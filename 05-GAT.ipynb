{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "num_train_nodes: 140 | num_val_nodes: 500 | num_test_nodes: 1000\n"
     ]
    }
   ],
   "source": [
    "# cora dataset = 1 graph of citation network: node classification dataset\n",
    "import dgl.data\n",
    "dataset=dgl.data.CoraGraphDataset()\n",
    "g=dataset[0]\n",
    "\n",
    "# train_mask, val_mask, test_mask = boolean indices for train, val, test\n",
    "train_mask=g.ndata['train_mask']\n",
    "val_mask=g.ndata['val_mask']\n",
    "test_mask=g.ndata['test_mask']\n",
    "\n",
    "print(f\"num_train_nodes: {sum(train_mask)} | num_val_nodes: {sum(val_mask)} | num_test_nodes: {sum(test_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters (used on models below)\n",
    "in_dim=g.ndata['feat'].shape[-1]\n",
    "hidden_dim=16\n",
    "out_dim=dataset.num_classes\n",
    "num_heads=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Graph Attention Network\n",
    "GCN combines local graph structure and node-level features. \n",
    "$$h_i^{(l+1)}=W\\sum_{j\\in N(i)}\\dfrac{1}{\\sqrt{\\deg(i)\\deg(j)}}h_j^{(l)}+b$$\n",
    "This approach yields good performance on node classification tasks. However, GCN only considers neighbor information and ignores the node itself. Such a structure dependent approach can hurt its generality (graph calssification).\n",
    "\n",
    "GraphSage remedies this issue by concatenating the node and the neighbor information.\n",
    "$$h_i^{(l+1)}= W\\text{concat}(h_i^{(l)},h_{N(i)}^{(l+1)})+b \\ \\text{with} \\ h_{N(i)}^{(l+1)}=\\text{Mean}\\{h_j^{(l)}: j\\in N(i)\\}$$\n",
    "\n",
    "GAT uses weighting neighbor features with feature dependent and structure-free normalization, in the style of attention, i.e. the attention coefficients are learnable!\n",
    "\n",
    "\\begin{align}\n",
    "h_i^{(l+1)} &=W\\sum_{j\\in N(i)}\\alpha_{ij}^{(l)}h_j^{(l)} \\ \\text{with} \\\\\n",
    "\\alpha_{ij}^{(l)}&=\\text{softmax}_j(e_{ik}^{(l)}: k \\in N(i)) \\\\\n",
    "e_{ij}^{(l)}&= \\text{LeakyReLU}\\left(a^{(l)T}\\cdot\\text{concat}(Wh_i^{(l)} , Wh_j^{(l)})\\right)\n",
    "\\end{align}\n",
    "Note: $e_{ij}$ = un-normalized attention of edge $\\{i,j\\}$, $\\alpha_{ij}$ = normalized attention coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GAT layer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Single-head GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class GAT_layer(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # matrix W\n",
    "        self.linear_proj=nn.Linear(in_dim,out_dim,bias=False)\n",
    "\n",
    "        # attention params a\n",
    "        self.attn_param=nn.Linear(2*out_dim,1,bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # initialize learnable parameters by xavier_normal with \"relu gain\"\n",
    "        nn.init.xavier_normal_(self.linear_proj.weight,gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_normal_(self.attn_param.weight,gain=nn.init.calculate_gain('relu'))\n",
    "    \n",
    "    # user-defined function for equation (3)\n",
    "    #              return {'e': e} stored in edges.data: via g.apply_edges\n",
    "    def edge_attention(self,edges): \n",
    "        # concatenate src and dst node features\n",
    "        concat=torch.cat([edges.src['W_h'], edges.dst['W_h']],dim=1) # [E,2d] \n",
    "        e=self.attn_param(concat)                                    # [E,out_dim]\n",
    "        return {'e': F.leaky_relu(e)}                                # [E,out_dim]\n",
    "\n",
    "    # message_func(self, edges): send info through edges\n",
    "    #                            return all information needed to update node features \n",
    "    #                            {'W_h':W_h, 'e': e} that is stored in nodes.mailbox\n",
    "    def message_func(self,edges):\n",
    "        return {'W_h': edges.src['W_h'], 'e': edges.data['e']}  # nodes.mailbox['W_h']=[E,in_dim]\n",
    "        \n",
    "\n",
    "    # reduce_func(self,nodes): update node features in equation (1)\n",
    "    #                          return {'h':h} that is stored in \n",
    "    def reduce_func(self,nodes):\n",
    "        # attention coefficients\n",
    "        alpha=F.softmax(nodes.mailbox['e'],dim=1)               # [E,out_dim]\n",
    "\n",
    "        # take weighted sum of the neighbors\n",
    "        h_N=torch.sum(alpha * nodes.mailbox['W_h'],dim=1)       # [E,out_dim]*[E,out_dim]=[E,out_dim]\n",
    "        \n",
    "        return {'h_N': h_N} # new node features\n",
    "        \n",
    "\n",
    "    def forward(self,g,h):\n",
    "        \n",
    "        with g.local_scope():\n",
    "\n",
    "            g.ndata['h']=h                                      # [N,in_dim]\n",
    "            \n",
    "            W_h=self.linear_proj(h)\n",
    "            g.ndata['W_h']=W_h                                   # [N,in_dim]\n",
    "            \n",
    "            # assign attention e_ij to every edges\n",
    "            g.apply_edges(self.edge_attention) \n",
    "\n",
    "            # send info through edges and update node features\n",
    "            g.update_all(self.message_func,self.reduce_func)\n",
    "\n",
    "            return g.ndata['h_N']\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "lay=GAT_layer(in_dim,out_dim)\n",
    "out=lay(g,g.ndata['feat'])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Multi-head GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGAT_Layer(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.heads=nn.ModuleList([GAT_layer(in_dim,out_dim) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        # concatenate individual outputs\n",
    "        out=torch.cat([self.heads[i](g,h) for i in range(self.num_heads)],dim=1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 14])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "layer_multihead=MultiHeadGAT_Layer(in_dim,out_dim,num_heads)\n",
    "output=layer_multihead(g,g.ndata['feat'])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Model\n",
    "input -> MultiHeadGAT_Layer1 -> elu -> MultiHeadGAT_Layer2 -> out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_Net(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.gat1=MultiHeadGAT_Layer(in_dim,hidden_dim,num_heads)    # output dim=hidden_dim*num_heads\n",
    "        self.gat2=MultiHeadGAT_Layer(hidden_dim*num_heads,out_dim,1) # output dim = out_dim*1=out_dim\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        h=self.gat1(g,h)\n",
    "        h=F.elu(h)\n",
    "        h=self.gat2(g,h)\n",
    "        return h\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "\n",
    "def train(model, graph, loss_fn,optimizer):\n",
    "    model.train()\n",
    "    features=graph.ndata['feat']\n",
    "    labels=graph.ndata['label']\n",
    "    \n",
    "    # forward and backward on train_mask    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # prediction on the whole graph\n",
    "    logits=model(graph, features) \n",
    "    \n",
    "    # only consider train_mask\n",
    "    loss=loss_fn(logits[train_mask],labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # compute accuracy\n",
    "    preds=logits.argmax(dim=-1)\n",
    "    acc=(preds[train_mask]==labels[train_mask]).float().mean()\n",
    "    return loss, acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model,graph, loss_fn,optimizer):\n",
    "    model.eval()\n",
    "    features=graph.ndata['feat']\n",
    "    labels=graph.ndata['label']\n",
    "\n",
    "    # forward\n",
    "    logits=model(graph,features)\n",
    "    loss=loss_fn(logits[val_mask],labels[val_mask])\n",
    "\n",
    "    # compute accuracy\n",
    "    preds=logits.argmax(dim=-1)\n",
    "    acc=(preds[val_mask]==labels[val_mask]).float().mean()\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046158 million parameters\n",
      "Epoch : 0 | train_loss : 1.9467 | train_acc : 12.86% |  val_loss : 1.8070 | val_acc : 73.40%\n",
      "Epoch : 5 | train_loss : 0.3415 | train_acc : 97.14% |  val_loss : 0.8541 | val_acc : 75.80%\n",
      "Epoch : 10 | train_loss : 0.0145 | train_acc : 100.00% |  val_loss : 1.2353 | val_acc : 75.60%\n",
      "Epoch : 15 | train_loss : 0.0016 | train_acc : 100.00% |  val_loss : 1.5936 | val_acc : 76.00%\n",
      "Epoch : 19 | train_loss : 0.0034 | train_acc : 100.00% |  val_loss : 1.7326 | val_acc : 76.40%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1442)\n",
    "\n",
    "num_epochs=20\n",
    "\n",
    "# model and optimizer\n",
    "\n",
    "model=GAT_Net(in_dim,hidden_dim,out_dim,num_heads)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\n",
    "\n",
    "loss_fn=F.cross_entropy\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.1)\n",
    "\n",
    "\n",
    "# train and test\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc=train(model,g,loss_fn,optimizer)\n",
    "    val_loss, val_acc=evaluate(model,g,loss_fn,optimizer)\n",
    "    if epoch%5==0 or epoch == num_epochs-1:\n",
    "        print(f\"Epoch : {epoch} | train_loss : {train_loss:.4f} | train_acc : {train_acc*100:.2f}% | \"\n",
    "            f\" val_loss : {val_loss:.4f} | val_acc : {val_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
