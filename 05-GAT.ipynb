{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "num_train_nodes: 140 | num_val_nodes: 500 | num_test_nodes: 1000\n"
     ]
    }
   ],
   "source": [
    "# cora dataset = 1 graph of citation network: node classification dataset\n",
    "import dgl.data\n",
    "dataset=dgl.data.CoraGraphDataset()\n",
    "g=dataset[0]\n",
    "\n",
    "# train_mask, val_mask, test_mask = boolean indices for train, val, test\n",
    "train_mask=g.ndata['train_mask']\n",
    "val_mask=g.ndata['val_mask']\n",
    "test_mask=g.ndata['test_mask']\n",
    "\n",
    "print(f\"num_train_nodes: {sum(train_mask)} | num_val_nodes: {sum(val_mask)} | num_test_nodes: {sum(test_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_dim: 1433 | hidden_dim: 16 | out_dim: 7 | num_heads: 2\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters (used on models below)\n",
    "in_dim=g.ndata['feat'].shape[-1]\n",
    "hidden_dim=16\n",
    "out_dim=dataset.num_classes\n",
    "num_heads=2\n",
    "print(f\"in_dim: {in_dim} | hidden_dim: {hidden_dim} | out_dim: {out_dim} | num_heads: {num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Graph Attention Network\n",
    "Preview on GCN and GraphSage:\n",
    "1. GCN update equation\n",
    "$$h_i^{(l+1)}=W\\sum_{j\\in N(i)}\\dfrac{1}{\\sqrt{\\deg(i)\\deg(j)}}h_j^{(l)}+b$$\n",
    "2. GraphSage update equation:\n",
    "$$h_i^{(l+1)}= W\\text{concat}(h_i^{(l)},h_{N(i)}^{(l+1)})+b \\ \\text{with} \\ h_{N(i)}^{(l+1)}=\\text{Mean}\\{h_j^{(l)}: j\\in N(i)\\}$$\n",
    "3. **GAT** update equation\n",
    "\\begin{align}\n",
    "h_i^{(l+1)} &=W\\sum_{j\\in N(i)}\\alpha_{ij}^{(l)}h_j^{(l)} \\ \\text{with} \\\\\n",
    "\\alpha_{ij}^{(l)}&=\\text{softmax}_j(e_{ik}^{(l)}: k \\in N(i)) \\\\\n",
    "e_{ij}^{(l)}&= \\text{LeakyReLU}\\left(a^{(l)T}\\cdot\\text{concat}(Wh_i^{(l)} , Wh_j^{(l)})\\right)\n",
    "\\end{align}\n",
    "with $e_{ij}$ = un-normalized attention of edge $\\{i,j\\}$, $\\alpha_{ij}$ = normalized attention coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GAT layer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Single-head GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "# flow: (1) Apply linear projection to all nodes via matrix W\n",
    "#       (2) Compute attention e_ij for all edges {i,j}\n",
    "#       (3) Send info through edges and update node features \n",
    "\n",
    "class GAT_layer(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # matrix W\n",
    "        self.linear_proj=nn.Linear(in_dim,out_dim,bias=False)\n",
    "\n",
    "        # attention params a\n",
    "        self.attn_param=nn.Linear(2*out_dim,1,bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # initialize learnable parameters by xavier_normal with \"relu gain\"\n",
    "        nn.init.xavier_normal_(self.linear_proj.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_normal_(self.attn_param.weight,  gain=nn.init.calculate_gain('relu'))\n",
    "    \n",
    "    # user-defined function for equation (3)\n",
    "    #              return dict {'e': e} stored in edges.data: via g.apply_edges\n",
    "    def edge_attention(self,edges): \n",
    "        # concatenate src and dst node features\n",
    "        concat=torch.cat([edges.src['W_h'], edges.dst['W_h']],dim=1) \n",
    "        e=self.attn_param(concat)                                    \n",
    "        return {'e': F.leaky_relu(e)}                                \n",
    "\n",
    "    # message_func(self, edges): send info through edges\n",
    "    #             return all information needed to update node features \n",
    "    #             {'W_h':W_h, 'e': e} is stored in nodes.mailbox\n",
    "    def message_func(self,edges):\n",
    "        return {'W_h': edges.src['W_h'], 'e': edges.data['e']}  # nodes.mailbox['W_h']=[E,in_dim]\n",
    "        \n",
    "\n",
    "    # reduce_func(self,nodes): update node features in equation (1)\n",
    "    #                          return {'h':h} that is stored in \n",
    "    def reduce_func(self,nodes):\n",
    "        # attention coefficients\n",
    "        alpha=F.softmax(nodes.mailbox['e'],dim=1)               \n",
    "\n",
    "        # take weighted sum of the neighbors\n",
    "        h_N=torch.sum(alpha * nodes.mailbox['W_h'],dim=1)       \n",
    "        \n",
    "        return {'h_N': h_N} # new node features\n",
    "        \n",
    "\n",
    "    def forward(self,g,h):\n",
    "        \n",
    "        with g.local_scope():\n",
    "\n",
    "            g.ndata['h']=h                                      # [N,in_dim]\n",
    "            \n",
    "            W_h=self.linear_proj(h)                             # [N,out_dim]\n",
    "            g.ndata['W_h']=W_h                                  # [N,out_dim]\n",
    "            \n",
    "            # compute attention e_ij to every edges\n",
    "            g.apply_edges(self.edge_attention) \n",
    "\n",
    "            # send info through edges and update node features\n",
    "            g.update_all(self.message_func,self.reduce_func)\n",
    "\n",
    "            return g.ndata['h_N']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "lay=GAT_layer(in_dim,out_dim)\n",
    "out=lay(g,g.ndata['feat'])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Multi-head GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGAT_Layer(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads=num_heads     # num_heads=2\n",
    "        self.heads=nn.ModuleList([GAT_layer(in_dim,out_dim) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        # concatenate individual outputs\n",
    "        out=torch.cat([self.heads[i](g,h) for i in range(self.num_heads)],dim=1)\n",
    "        return out  # [N,out_dim*num_heads]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 14])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "layer_multihead=MultiHeadGAT_Layer(in_dim,out_dim,num_heads)\n",
    "output=layer_multihead(g,g.ndata['feat'])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Model\n",
    "input -> MultiHeadGAT_Layer1 -> elu -> MultiHeadGAT_Layer2 -> out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_Net(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.gat1=MultiHeadGAT_Layer(in_dim,hidden_dim,num_heads)    # [N,hidden_dim*num_heads]\n",
    "        self.gat2=MultiHeadGAT_Layer(hidden_dim*num_heads,out_dim,1) # [N,out_dim*1]\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        h=self.gat1(g,h)\n",
    "        h=F.elu(h)\n",
    "        h=self.gat2(g,h)\n",
    "        return h\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "\n",
    "def train(model, graph, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    features=graph.ndata['feat']\n",
    "    labels=graph.ndata['label']\n",
    "    \n",
    "    # forward and backward on train_mask    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # prediction on the whole graph\n",
    "    logits=model(graph, features) \n",
    "    \n",
    "    # only consider train_mask\n",
    "    loss=loss_fn(logits[train_mask],labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # compute accuracy\n",
    "    preds=logits.argmax(dim=-1)\n",
    "    acc=(preds[train_mask]==labels[train_mask]).float().mean()\n",
    "    return loss, acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, graph, loss_fn):\n",
    "    model.eval()\n",
    "    features=graph.ndata['feat']\n",
    "    labels=graph.ndata['label']\n",
    "\n",
    "    # forward\n",
    "    logits=model(graph,features)\n",
    "    loss=loss_fn(logits[val_mask],labels[val_mask])\n",
    "\n",
    "    # compute accuracy\n",
    "    preds=logits.argmax(dim=-1)\n",
    "    acc=(preds[val_mask]==labels[val_mask]).float().mean()\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046158 million parameters\n",
      "Epoch : 0 | train_loss : 1.9467 | train_acc : 12.86% |  val_loss : 1.9308 | val_acc : 56.40%\n",
      "Epoch : 5 | train_loss : 1.7944 | train_acc : 95.71% |  val_loss : 1.8511 | val_acc : 73.40%\n",
      "Epoch : 10 | train_loss : 1.6095 | train_acc : 95.71% |  val_loss : 1.7515 | val_acc : 74.60%\n",
      "Epoch : 15 | train_loss : 1.3920 | train_acc : 95.71% |  val_loss : 1.6309 | val_acc : 74.40%\n",
      "Epoch : 20 | train_loss : 1.1520 | train_acc : 96.43% |  val_loss : 1.4909 | val_acc : 75.40%\n",
      "Epoch : 25 | train_loss : 0.9071 | train_acc : 96.43% |  val_loss : 1.3380 | val_acc : 75.60%\n",
      "Epoch : 30 | train_loss : 0.6785 | train_acc : 97.14% |  val_loss : 1.1865 | val_acc : 76.20%\n",
      "Epoch : 35 | train_loss : 0.4849 | train_acc : 98.57% |  val_loss : 1.0557 | val_acc : 76.60%\n",
      "Epoch : 40 | train_loss : 0.3351 | train_acc : 98.57% |  val_loss : 0.9618 | val_acc : 76.80%\n",
      "Epoch : 45 | train_loss : 0.2252 | train_acc : 99.29% |  val_loss : 0.9103 | val_acc : 77.20%\n",
      "Epoch : 49 | train_loss : 0.1639 | train_acc : 99.29% |  val_loss : 0.8909 | val_acc : 76.40%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1442)\n",
    "\n",
    "num_epochs=50\n",
    "\n",
    "# model and optimizer\n",
    "\n",
    "model=GAT_Net(in_dim,hidden_dim,out_dim,num_heads)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\n",
    "\n",
    "loss_fn=F.cross_entropy\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.01)\n",
    "\n",
    "\n",
    "# train and test\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc=train(model,g,loss_fn,optimizer)\n",
    "    val_loss, val_acc=evaluate(model,g,loss_fn)\n",
    "    if epoch%5==0 or epoch == num_epochs-1:\n",
    "        print(f\"Epoch : {epoch} | train_loss : {train_loss:.4f} | train_acc : {train_acc*100:.2f}% | \"\n",
    "            f\" val_loss : {val_loss:.4f} | val_acc : {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. GAT from dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "class GAT_DGL(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.gat1=GATConv(in_dim,hidden_dim,num_heads)    # [num_nodes,num_heads,hidden_dim]\n",
    "        self.gat2=GATConv(hidden_dim*num_heads,out_dim,1) # [num_nodes,1,out_dim]\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        h=self.gat1(g,h)\n",
    "        h=h.reshape(-1,h.shape[-2]*h.shape[-1]) # [num_nodes,num_heads*hidden_dim]\n",
    "        h=F.elu(h)\n",
    "        h=self.gat2(g,h)                        # [num_nodes,1,out_dim]\n",
    "        h=h.squeeze(dim=-2)                     # [num_nodes,out_dim]\n",
    "        return h\n",
    "    \n",
    "# sanity check\n",
    "gat_dgl=GAT_DGL(in_dim,hidden_dim,out_dim,num_heads)\n",
    "print(g.ndata['feat'].shape)\n",
    "output=gat_dgl(g,g.ndata['feat'])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046197 million parameters\n",
      "Epoch : 0 | train_loss : 1.9469 | train_acc : 15.00% |  val_loss : 1.9190 | val_acc : 34.00%\n",
      "Epoch : 5 | train_loss : 1.7943 | train_acc : 97.86% |  val_loss : 1.8468 | val_acc : 76.40%\n",
      "Epoch : 10 | train_loss : 1.6067 | train_acc : 96.43% |  val_loss : 1.7410 | val_acc : 77.60%\n",
      "Epoch : 15 | train_loss : 1.3828 | train_acc : 97.86% |  val_loss : 1.6176 | val_acc : 77.20%\n",
      "Epoch : 20 | train_loss : 1.1312 | train_acc : 97.86% |  val_loss : 1.4723 | val_acc : 77.00%\n",
      "Epoch : 25 | train_loss : 0.8702 | train_acc : 97.86% |  val_loss : 1.3191 | val_acc : 77.00%\n",
      "Epoch : 30 | train_loss : 0.6273 | train_acc : 98.57% |  val_loss : 1.1743 | val_acc : 77.80%\n",
      "Epoch : 35 | train_loss : 0.4271 | train_acc : 99.29% |  val_loss : 1.0554 | val_acc : 77.60%\n",
      "Epoch : 40 | train_loss : 0.2775 | train_acc : 99.29% |  val_loss : 0.9740 | val_acc : 77.00%\n",
      "Epoch : 45 | train_loss : 0.1740 | train_acc : 100.00% |  val_loss : 0.9282 | val_acc : 75.60%\n",
      "Epoch : 49 | train_loss : 0.1182 | train_acc : 100.00% |  val_loss : 0.9121 | val_acc : 75.00%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1442)\n",
    "\n",
    "num_epochs=50\n",
    "\n",
    "# model and optimizer\n",
    "\n",
    "model=GAT_DGL(in_dim,hidden_dim,out_dim,num_heads)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\n",
    "\n",
    "loss_fn=F.cross_entropy\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.01)\n",
    "\n",
    "\n",
    "# train and test\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc=train(model,g,loss_fn,optimizer)\n",
    "    val_loss, val_acc=evaluate(model,g,loss_fn)\n",
    "    if epoch%5==0 or epoch == num_epochs-1:\n",
    "        print(f\"Epoch : {epoch} | train_loss : {train_loss:.4f} | train_acc : {train_acc*100:.2f}% | \"\n",
    "            f\" val_loss : {val_loss:.4f} | val_acc : {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
