{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "num_train_nodes: 140 | num_val_nodes: 500 | num_test_nodes: 1000\n"
     ]
    }
   ],
   "source": [
    "# cora dataset = 1 graph of citation network: node classification dataset\n",
    "import dgl.data\n",
    "dataset=dgl.data.CoraGraphDataset()\n",
    "g=dataset[0]\n",
    "\n",
    "# train_mask, val_mask, test_mask = boolean indices for train, val, test\n",
    "train_mask=g.ndata['train_mask']\n",
    "val_mask=g.ndata['val_mask']\n",
    "test_mask=g.ndata['test_mask']\n",
    "\n",
    "print(f\"num_train_nodes: {sum(train_mask)} | num_val_nodes: {sum(val_mask)} | num_test_nodes: {sum(test_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_dim: 1433 | hidden_dim: 16 | out_dim: 7 | num_heads: 2\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters (used on models below)\n",
    "in_dim=g.ndata['feat'].shape[-1]\n",
    "hidden_dim=16\n",
    "out_dim=dataset.num_classes\n",
    "num_heads=2\n",
    "print(f\"in_dim: {in_dim} | hidden_dim: {hidden_dim} | out_dim: {out_dim} | num_heads: {num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Graph Attention Network\n",
    "Preview on GCN and GraphSage:\n",
    "1. GCN update equation\n",
    "$$h_i^{(l+1)}=W\\sum_{j\\in N(i)}\\dfrac{1}{\\sqrt{\\deg(i)\\deg(j)}}h_j^{(l)}+b$$\n",
    "2. GraphSage update equation:\n",
    "$$h_i^{(l+1)}= W\\text{concat}(h_i^{(l)},h_{N(i)}^{(l+1)})+b \\ \\text{with} \\ h_{N(i)}^{(l+1)}=\\text{Mean}\\{h_j^{(l)}: j\\in N(i)\\}$$\n",
    "3. **GAT** update equation\n",
    "\\begin{align}\n",
    "h_i^{(l+1)} &=W\\sum_{j\\in N(i)}\\alpha_{ij}^{(l)}h_j^{(l)} \\ \\text{with} \\\\\n",
    "\\alpha_{ij}^{(l)}&=\\text{softmax}_j(e_{ik}^{(l)}: k \\in N(i)) \\\\\n",
    "e_{ij}^{(l)}&= \\text{LeakyReLU}\\left(a^{(l)T}\\cdot\\text{concat}(Wh_i^{(l)} , Wh_j^{(l)})\\right)\n",
    "\\end{align}\n",
    "with $e_{ij}$ = un-normalized attention of edge $\\{i,j\\}$, $\\alpha_{ij}$ = normalized attention coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GAT layer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Single-head GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class GAT_layer(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # matrix W\n",
    "        self.linear_proj=nn.Linear(in_dim,out_dim,bias=False)\n",
    "\n",
    "        # attention params a\n",
    "        self.attn_param=nn.Linear(2*out_dim,1,bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # initialize learnable parameters by xavier_normal with \"relu gain\"\n",
    "        nn.init.xavier_normal_(self.linear_proj.weight,gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_normal_(self.attn_param.weight,gain=nn.init.calculate_gain('relu'))\n",
    "    \n",
    "    # user-defined function for equation (3)\n",
    "    #              return {'e': e} stored in edges.data: via g.apply_edges\n",
    "    def edge_attention(self,edges): \n",
    "        # concatenate src and dst node features\n",
    "        concat=torch.cat([edges.src['W_h'], edges.dst['W_h']],dim=1) # [E,2d] \n",
    "        e=self.attn_param(concat)                                    # [E,out_dim]\n",
    "        return {'e': F.leaky_relu(e)}                                # [E,out_dim]\n",
    "\n",
    "    # message_func(self, edges): send info through edges\n",
    "    #             return all information needed to update node features \n",
    "    #             {'W_h':W_h, 'e': e} that is stored in nodes.mailbox\n",
    "    def message_func(self,edges):\n",
    "        return {'W_h': edges.src['W_h'], 'e': edges.data['e']}  # nodes.mailbox['W_h']=[E,in_dim]\n",
    "        \n",
    "\n",
    "    # reduce_func(self,nodes): update node features in equation (1)\n",
    "    #                          return {'h':h} that is stored in \n",
    "    def reduce_func(self,nodes):\n",
    "        # attention coefficients\n",
    "        alpha=F.softmax(nodes.mailbox['e'],dim=1)               # [E,out_dim]\n",
    "\n",
    "        # take weighted sum of the neighbors\n",
    "        h_N=torch.sum(alpha * nodes.mailbox['W_h'],dim=1)       # [E,out_dim]*[E,out_dim]=[E,out_dim]\n",
    "        \n",
    "        return {'h_N': h_N} # new node features\n",
    "        \n",
    "\n",
    "    def forward(self,g,h):\n",
    "        \n",
    "        with g.local_scope():\n",
    "\n",
    "            g.ndata['h']=h                                      # [N,in_dim]\n",
    "            \n",
    "            W_h=self.linear_proj(h)\n",
    "            g.ndata['W_h']=W_h                                   # [N,in_dim]\n",
    "            \n",
    "            # assign attention e_ij to every edges\n",
    "            g.apply_edges(self.edge_attention) \n",
    "\n",
    "            # send info through edges and update node features\n",
    "            g.update_all(self.message_func,self.reduce_func)\n",
    "\n",
    "            return g.ndata['h_N']\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "lay=GAT_layer(in_dim,out_dim)\n",
    "out=lay(g,g.ndata['feat'])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Multi-head GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGAT_Layer(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.heads=nn.ModuleList([GAT_layer(in_dim,out_dim) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        # concatenate individual outputs\n",
    "        out=torch.cat([self.heads[i](g,h) for i in range(self.num_heads)],dim=1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 14])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "layer_multihead=MultiHeadGAT_Layer(in_dim,out_dim,num_heads)\n",
    "output=layer_multihead(g,g.ndata['feat'])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Model\n",
    "input -> MultiHeadGAT_Layer1 -> elu -> MultiHeadGAT_Layer2 -> out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_Net(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.gat1=MultiHeadGAT_Layer(in_dim,hidden_dim,num_heads)    # output dim=hidden_dim*num_heads\n",
    "        self.gat2=MultiHeadGAT_Layer(hidden_dim*num_heads,out_dim,1) # output dim = out_dim*1=out_dim\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        h=self.gat1(g,h)\n",
    "        h=F.elu(h)\n",
    "        h=self.gat2(g,h)\n",
    "        return h\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "\n",
    "def train(model, graph, loss_fn,optimizer):\n",
    "    model.train()\n",
    "    features=graph.ndata['feat']\n",
    "    labels=graph.ndata['label']\n",
    "    \n",
    "    # forward and backward on train_mask    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # prediction on the whole graph\n",
    "    logits=model(graph, features) \n",
    "    \n",
    "    # only consider train_mask\n",
    "    loss=loss_fn(logits[train_mask],labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # compute accuracy\n",
    "    preds=logits.argmax(dim=-1)\n",
    "    acc=(preds[train_mask]==labels[train_mask]).float().mean()\n",
    "    return loss, acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model,graph, loss_fn,optimizer):\n",
    "    model.eval()\n",
    "    features=graph.ndata['feat']\n",
    "    labels=graph.ndata['label']\n",
    "\n",
    "    # forward\n",
    "    logits=model(graph,features)\n",
    "    loss=loss_fn(logits[val_mask],labels[val_mask])\n",
    "\n",
    "    # compute accuracy\n",
    "    preds=logits.argmax(dim=-1)\n",
    "    acc=(preds[val_mask]==labels[val_mask]).float().mean()\n",
    "    return loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046158 million parameters\n",
      "Epoch : 0 | train_loss : 1.9467 | train_acc : 12.86% |  val_loss : 1.8070 | val_acc : 73.40%\n",
      "Epoch : 5 | train_loss : 0.3415 | train_acc : 97.14% |  val_loss : 0.8541 | val_acc : 75.80%\n",
      "Epoch : 10 | train_loss : 0.0145 | train_acc : 100.00% |  val_loss : 1.2353 | val_acc : 75.60%\n",
      "Epoch : 15 | train_loss : 0.0016 | train_acc : 100.00% |  val_loss : 1.5936 | val_acc : 76.00%\n",
      "Epoch : 19 | train_loss : 0.0034 | train_acc : 100.00% |  val_loss : 1.7326 | val_acc : 76.40%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1442)\n",
    "\n",
    "num_epochs=20\n",
    "\n",
    "# model and optimizer\n",
    "\n",
    "model=GAT_Net(in_dim,hidden_dim,out_dim,num_heads)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\n",
    "\n",
    "loss_fn=F.cross_entropy\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.1)\n",
    "\n",
    "\n",
    "# train and test\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc=train(model,g,loss_fn,optimizer)\n",
    "    val_loss, val_acc=evaluate(model,g,loss_fn,optimizer)\n",
    "    if epoch%5==0 or epoch == num_epochs-1:\n",
    "        print(f\"Epoch : {epoch} | train_loss : {train_loss:.4f} | train_acc : {train_acc*100:.2f}% | \"\n",
    "            f\" val_loss : {val_loss:.4f} | val_acc : {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. GAT from dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "class GAT_DGL(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.gat1=GATConv(in_dim,hidden_dim,num_heads)    # output dim=hidden_dim*num_heads\n",
    "        self.gat2=GATConv(hidden_dim*num_heads,out_dim,1) # output dim = out_dim*1=out_dim\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        h=self.gat1(g,h)\n",
    "        h=F.elu(h)\n",
    "        h=self.gat2(g,h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046197 million parameters\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (5416x16 and 32x7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# train and test\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 16\u001b[0m     train_loss, train_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     val_loss, val_acc\u001b[38;5;241m=\u001b[39mevaluate(model,g,loss_fn,optimizer)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m num_epochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, graph, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# prediction on the whole graph\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m logits\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# only consider train_mask\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss_fn(logits[train_mask],labels[train_mask])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m, in \u001b[0;36mGAT_DGL.forward\u001b[0;34m(self, g, h)\u001b[0m\n\u001b[1;32m     10\u001b[0m h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat1(g,h)\n\u001b[1;32m     11\u001b[0m h\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39melu(h)\n\u001b[0;32m---> 12\u001b[0m h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/dgl/nn/pytorch/conv/gatconv.py:313\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, graph, feat, edge_weight, get_attention)\u001b[0m\n\u001b[1;32m    311\u001b[0m src_prefix_shape \u001b[38;5;241m=\u001b[39m dst_prefix_shape \u001b[38;5;241m=\u001b[39m feat\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    312\u001b[0m h_src \u001b[38;5;241m=\u001b[39m h_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeat_drop(feat)\n\u001b[0;32m--> 313\u001b[0m feat_src \u001b[38;5;241m=\u001b[39m feat_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_src\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;241m*\u001b[39msrc_prefix_shape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_feats\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mis_block:\n\u001b[1;32m    317\u001b[0m     feat_dst \u001b[38;5;241m=\u001b[39m feat_src[: graph\u001b[38;5;241m.\u001b[39mnumber_of_dst_nodes()]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5416x16 and 32x7)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1442)\n",
    "\n",
    "num_epochs=20\n",
    "\n",
    "# model and optimizer\n",
    "\n",
    "model=GAT_DGL(in_dim,hidden_dim,out_dim,num_heads)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\n",
    "\n",
    "loss_fn=F.cross_entropy\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.1)\n",
    "\n",
    "\n",
    "# train and test\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc=train(model,g,loss_fn,optimizer)\n",
    "    val_loss, val_acc=evaluate(model,g,loss_fn,optimizer)\n",
    "    if epoch%5==0 or epoch == num_epochs-1:\n",
    "        print(f\"Epoch : {epoch} | train_loss : {train_loss:.4f} | train_acc : {train_acc*100:.2f}% | \"\n",
    "            f\" val_loss : {val_loss:.4f} | val_acc : {val_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
