{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "num_train_nodes: 140 | num_val_nodes: 500 | num_test_nodes: 1000\n"
     ]
    }
   ],
   "source": [
    "# cora dataset = 1 graph of citation network\n",
    "#        nodes = papers\n",
    "#        edges = connectivity (citation) between papers\n",
    "\n",
    "import dgl.data\n",
    "\n",
    "dataset=dgl.data.CoraGraphDataset()\n",
    "g=dataset[0]\n",
    "\n",
    "# train_mask, val_mask, test_mask= boolean indices for train, val, test\n",
    "train_mask=g.ndata['train_mask']\n",
    "val_mask=g.ndata['val_mask']\n",
    "test_mask=g.ndata['test_mask']\n",
    "\n",
    "print(f\"num_train_nodes: {sum(train_mask)} | num_val_nodes: {sum(val_mask)} | num_test_nodes: {sum(test_mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGL framework\n",
    "DGL updates node features via 3 steps:\n",
    "\n",
    "1. **message_func(edges)**: \n",
    "Each edge has attributres edge=[src, dst, data]. This function sends info $$src \\rightarrow dst$$ It stores everything needed to do node-feature update in a dict called \"mailbox\". \n",
    "2. **reduce_func(nodes)**: Update the node features by the update equation. All info in \"mailbox\" (obtained from message_func) will be used in this step.\n",
    "3. **update_all(message_func,reduce_func)**: send messages through all edges (by message_func) and update features of all nodes (by reduce_func)\n",
    "\n",
    "Additionally if the update equations involve edge updates, use **apply_edges(func)** to update edge features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GraphSage\n",
    "GraphSage is a convolution layer in a graph neural network (GNN) which updates as follows \n",
    "\\begin{align*}\n",
    "h_i^{(l+1)}&= W.\\text{concat}(h_i^{(l)},h_{N(i)}^{(l+1)})+b \\ \\text{with} \\\\\n",
    " h_{N(i)}^{(l+1)}&=\\text{Mean}\\left(h_j^{(l)}: j\\in N(i)\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Recall: update equation for GCN\n",
    "$$h_i^{(l+1)}=W^{(l)}\\sum_{j\\in N(i)}\\dfrac{1}{c_{ji}}h_j^{(l)}+b^{(l)}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class sageconv(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(2*in_dim,out_dim)\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        # g=input graph, h=input feature nodes\n",
    "        with g.local_scope():\n",
    "            g.ndata['h']=h\n",
    "            # create a new feature \"h_N\" that takes the mean of neighbor nodes\n",
    "            g.update_all(message_func=fn.copy_u('h','m'), reduce_func=fn.mean('m','h_N'))\n",
    "            h_N=g.ndata['h_N'] # h_N(i)\n",
    "            h_concat=torch.cat([h,h_N],dim=-1)\n",
    "            return self.linear(h_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "in_dim=g.ndata[\"feat\"].shape[-1]\n",
    "hidden_dim=64\n",
    "out_dim=dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.184391 million parameters\n",
      "torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "# GNN: input -> sageconv1 -> relu -> sageconv2 -> classification\n",
    "\n",
    "class sageconv_net(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.conv1=sageconv(in_dim,hidden_dim)\n",
    "        self.conv2=sageconv(hidden_dim,out_dim)\n",
    "\n",
    "\n",
    "    def forward(self,g,h):\n",
    "        h=self.conv1(g,h)\n",
    "        h=F.relu(h)\n",
    "        h=self.conv2(g,h)\n",
    "        return h\n",
    "    \n",
    "net=sageconv_net(in_dim,hidden_dim,out_dim)\n",
    "print(f\"{sum(p.numel() for p in net.parameters())/1e6} million parameters\")\n",
    "\n",
    "# do 1 forward pass\n",
    "with torch.no_grad():\n",
    "    out=net(g,g.ndata[\"feat\"])\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "def train(model,graph,loss_fn,optimizer):\n",
    "    model.train()\n",
    "    features=graph.ndata['feat']\n",
    "    labels=graph.ndata['label']\n",
    "\n",
    "    # forward and backward\n",
    "    optimizer.zero_grad()\n",
    "    logits=model(graph,features) # prediction on the whole graph\n",
    "    # loss on train nodes\n",
    "    loss=loss_fn(logits[train_mask],labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # compute \n",
    "    preds=logits.argmax(dim=-1)\n",
    "    acc=(preds[train_mask]==labels[train_mask]).float().mean()\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "# evaluation loop\n",
    "@torch.no_grad\n",
    "def evaluate(model, graph, loss_fn):\n",
    "    model.eval()\n",
    "    features=graph.ndata['feat']\n",
    "    labels=graph.ndata['label']\n",
    "    # forward\n",
    "    logits=model(graph,features) # prediction on the whole graph\n",
    "    loss=loss_fn(logits[val_mask],labels[val_mask])\n",
    "    # compute acc\n",
    "    preds=logits.argmax(dim=-1)\n",
    "    acc=(preds[val_mask]==labels[val_mask]).float().mean()\n",
    "\n",
    "    return loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.9479 | train_acc: 14.29% | val_loss: 1.9358 | val_acc: 11.40% |\n",
      "Epoch: 2 | train_loss: 1.9331 | train_acc: 20.00% | val_loss: 1.9284 | val_acc: 16.80% |\n",
      "Epoch: 3 | train_loss: 1.9049 | train_acc: 39.29% | val_loss: 1.9184 | val_acc: 40.40% |\n",
      "Epoch: 4 | train_loss: 1.8689 | train_acc: 97.14% | val_loss: 1.9050 | val_acc: 53.60% |\n",
      "Epoch: 5 | train_loss: 1.8249 | train_acc: 99.29% | val_loss: 1.8872 | val_acc: 52.00% |\n",
      "Epoch: 6 | train_loss: 1.7717 | train_acc: 98.57% | val_loss: 1.8640 | val_acc: 52.60% |\n",
      "Epoch: 7 | train_loss: 1.7087 | train_acc: 98.57% | val_loss: 1.8348 | val_acc: 57.00% |\n",
      "Epoch: 8 | train_loss: 1.6358 | train_acc: 99.29% | val_loss: 1.7991 | val_acc: 61.00% |\n",
      "Epoch: 9 | train_loss: 1.5530 | train_acc: 99.29% | val_loss: 1.7567 | val_acc: 64.60% |\n",
      "Epoch: 10 | train_loss: 1.4608 | train_acc: 99.29% | val_loss: 1.7082 | val_acc: 66.20% |\n",
      "Epoch: 11 | train_loss: 1.3600 | train_acc: 99.29% | val_loss: 1.6544 | val_acc: 69.00% |\n",
      "Epoch: 12 | train_loss: 1.2520 | train_acc: 99.29% | val_loss: 1.5961 | val_acc: 70.20% |\n",
      "Epoch: 13 | train_loss: 1.1384 | train_acc: 99.29% | val_loss: 1.5339 | val_acc: 72.40% |\n",
      "Epoch: 14 | train_loss: 1.0213 | train_acc: 99.29% | val_loss: 1.4688 | val_acc: 73.60% |\n",
      "Epoch: 15 | train_loss: 0.9032 | train_acc: 99.29% | val_loss: 1.4018 | val_acc: 74.60% |\n",
      "Epoch: 16 | train_loss: 0.7867 | train_acc: 99.29% | val_loss: 1.3337 | val_acc: 75.40% |\n",
      "Epoch: 17 | train_loss: 0.6749 | train_acc: 99.29% | val_loss: 1.2657 | val_acc: 76.40% |\n",
      "Epoch: 18 | train_loss: 0.5702 | train_acc: 99.29% | val_loss: 1.1992 | val_acc: 77.00% |\n",
      "Epoch: 19 | train_loss: 0.4748 | train_acc: 99.29% | val_loss: 1.1359 | val_acc: 76.80% |\n",
      "Epoch: 20 | train_loss: 0.3901 | train_acc: 99.29% | val_loss: 1.0769 | val_acc: 76.40% |\n",
      "Epoch: 21 | train_loss: 0.3168 | train_acc: 99.29% | val_loss: 1.0232 | val_acc: 76.80% |\n",
      "Epoch: 22 | train_loss: 0.2548 | train_acc: 100.00% | val_loss: 0.9749 | val_acc: 76.40% |\n",
      "Epoch: 23 | train_loss: 0.2034 | train_acc: 100.00% | val_loss: 0.9320 | val_acc: 76.80% |\n",
      "Epoch: 24 | train_loss: 0.1615 | train_acc: 100.00% | val_loss: 0.8943 | val_acc: 76.60% |\n",
      "Epoch: 25 | train_loss: 0.1278 | train_acc: 100.00% | val_loss: 0.8613 | val_acc: 76.60% |\n",
      "Epoch: 26 | train_loss: 0.1009 | train_acc: 100.00% | val_loss: 0.8328 | val_acc: 76.60% |\n",
      "Epoch: 27 | train_loss: 0.0797 | train_acc: 100.00% | val_loss: 0.8086 | val_acc: 76.80% |\n",
      "Epoch: 28 | train_loss: 0.0630 | train_acc: 100.00% | val_loss: 0.7881 | val_acc: 76.80% |\n",
      "Epoch: 29 | train_loss: 0.0500 | train_acc: 100.00% | val_loss: 0.7713 | val_acc: 76.80% |\n",
      "Epoch: 30 | train_loss: 0.0398 | train_acc: 100.00% | val_loss: 0.7575 | val_acc: 77.00% |\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1442)\n",
    "\n",
    "# model, loss, optimizer\n",
    "model=sageconv_net(in_dim,hidden_dim,out_dim)\n",
    "\n",
    "# loss\n",
    "loss_fn=F.cross_entropy\n",
    "\n",
    "# optimzizer\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.01)\n",
    "\n",
    "num_epochs=30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc= train(model, g, loss_fn, optimizer)\n",
    "    val_loss, val_acc= evaluate(model,g,loss_fn)\n",
    "    print(f\"Epoch: {epoch+1} | train_loss: {train_loss:.4f} | train_acc: {train_acc*100:.2f}% | val_loss: {val_loss:.4f} | val_acc: {val_acc*100:.2f}% |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GraphSage with built-in function\n",
    "SAGEConv(in_feats, out_feats, aggregator_type, feat_drop=0.0, bias=True, norm=None, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.184391 million parameters\n"
     ]
    }
   ],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# model: input -> sageconv1 -> relu -> sageconv2 -> classification\n",
    "\n",
    "class SageConv_Net(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.conv1=SAGEConv(in_dim,hidden_dim,aggregator_type='mean')\n",
    "        self.conv2=SAGEConv(hidden_dim,out_dim,aggregator_type='mean')\n",
    "    \n",
    "    def forward(self,g,h):\n",
    "        h=self.conv1(g,h)\n",
    "        h=F.relu(h)\n",
    "        h=self.conv2(g,h)\n",
    "        return h\n",
    "    \n",
    "model2=SageConv_Net(in_dim,hidden_dim, out_dim)\n",
    "print(f\"{sum(p.numel() for p in model2.parameters())/1e6} million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.9485 | train_acc: 15.00% | val_loss: 1.9308 | val_acc: 24.00% |\n",
      "Epoch: 2 | train_loss: 1.8670 | train_acc: 67.86% | val_loss: 1.8802 | val_acc: 61.80% |\n",
      "Epoch: 3 | train_loss: 1.7599 | train_acc: 99.29% | val_loss: 1.8169 | val_acc: 71.20% |\n",
      "Epoch: 4 | train_loss: 1.6320 | train_acc: 99.29% | val_loss: 1.7495 | val_acc: 70.80% |\n",
      "Epoch: 5 | train_loss: 1.4933 | train_acc: 99.29% | val_loss: 1.6821 | val_acc: 70.60% |\n",
      "Epoch: 6 | train_loss: 1.3481 | train_acc: 99.29% | val_loss: 1.6144 | val_acc: 72.40% |\n",
      "Epoch: 7 | train_loss: 1.1980 | train_acc: 99.29% | val_loss: 1.5471 | val_acc: 74.20% |\n",
      "Epoch: 8 | train_loss: 1.0470 | train_acc: 99.29% | val_loss: 1.4804 | val_acc: 76.60% |\n",
      "Epoch: 9 | train_loss: 0.8995 | train_acc: 99.29% | val_loss: 1.4144 | val_acc: 78.20% |\n",
      "Epoch: 10 | train_loss: 0.7596 | train_acc: 99.29% | val_loss: 1.3488 | val_acc: 78.20% |\n",
      "Epoch: 11 | train_loss: 0.6306 | train_acc: 99.29% | val_loss: 1.2839 | val_acc: 78.60% |\n",
      "Epoch: 12 | train_loss: 0.5148 | train_acc: 99.29% | val_loss: 1.2203 | val_acc: 78.00% |\n",
      "Epoch: 13 | train_loss: 0.4138 | train_acc: 99.29% | val_loss: 1.1587 | val_acc: 77.60% |\n",
      "Epoch: 14 | train_loss: 0.3283 | train_acc: 99.29% | val_loss: 1.1000 | val_acc: 78.00% |\n",
      "Epoch: 15 | train_loss: 0.2577 | train_acc: 99.29% | val_loss: 1.0450 | val_acc: 77.80% |\n",
      "Epoch: 16 | train_loss: 0.2007 | train_acc: 100.00% | val_loss: 0.9943 | val_acc: 78.00% |\n",
      "Epoch: 17 | train_loss: 0.1554 | train_acc: 100.00% | val_loss: 0.9487 | val_acc: 77.80% |\n",
      "Epoch: 18 | train_loss: 0.1200 | train_acc: 100.00% | val_loss: 0.9084 | val_acc: 77.80% |\n",
      "Epoch: 19 | train_loss: 0.0926 | train_acc: 100.00% | val_loss: 0.8738 | val_acc: 77.80% |\n",
      "Epoch: 20 | train_loss: 0.0716 | train_acc: 100.00% | val_loss: 0.8446 | val_acc: 78.00% |\n",
      "Epoch: 21 | train_loss: 0.0556 | train_acc: 100.00% | val_loss: 0.8204 | val_acc: 78.20% |\n",
      "Epoch: 22 | train_loss: 0.0434 | train_acc: 100.00% | val_loss: 0.8005 | val_acc: 78.20% |\n",
      "Epoch: 23 | train_loss: 0.0342 | train_acc: 100.00% | val_loss: 0.7845 | val_acc: 78.40% |\n",
      "Epoch: 24 | train_loss: 0.0271 | train_acc: 100.00% | val_loss: 0.7716 | val_acc: 78.40% |\n",
      "Epoch: 25 | train_loss: 0.0217 | train_acc: 100.00% | val_loss: 0.7613 | val_acc: 78.40% |\n",
      "Epoch: 26 | train_loss: 0.0176 | train_acc: 100.00% | val_loss: 0.7531 | val_acc: 78.40% |\n",
      "Epoch: 27 | train_loss: 0.0143 | train_acc: 100.00% | val_loss: 0.7466 | val_acc: 78.40% |\n",
      "Epoch: 28 | train_loss: 0.0118 | train_acc: 100.00% | val_loss: 0.7416 | val_acc: 78.20% |\n",
      "Epoch: 29 | train_loss: 0.0098 | train_acc: 100.00% | val_loss: 0.7377 | val_acc: 78.20% |\n",
      "Epoch: 30 | train_loss: 0.0083 | train_acc: 100.00% | val_loss: 0.7347 | val_acc: 78.00% |\n"
     ]
    }
   ],
   "source": [
    "# model, loss, optimizer\n",
    "model2=SageConv_Net(in_dim,hidden_dim, out_dim)\n",
    "loss_fn=F.cross_entropy\n",
    "optimizer=torch.optim.AdamW(model2.parameters(),lr=0.01)\n",
    "\n",
    "num_epochs=30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc= train(model2, g, loss_fn, optimizer)\n",
    "    val_loss, val_acc= evaluate(model2,g,loss_fn)\n",
    "    print(f\"Epoch: {epoch+1} | train_loss: {train_loss:.4f} | train_acc: {train_acc*100:.2f}% | val_loss: {val_loss:.4f} | val_acc: {val_acc*100:.2f}% |\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
