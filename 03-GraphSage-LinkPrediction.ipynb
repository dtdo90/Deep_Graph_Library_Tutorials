{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Link prediction with GNN\n",
    "Assume we are given a graph g with incomplete data, for example, only 50% of the edges are present. \n",
    "\n",
    "The goal is to predict **whether there is an edge** between any 2 nodes in g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "# Load graph\n",
    "import dgl.data\n",
    "dataset=dgl.data.CoraGraphDataset()\n",
    "g=dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Prepare graph\n",
    "The graph we work **g_main** on is formed from the original graph by taking out 20% of its edges (test edges). \n",
    "1. **Training**: train g_main on known positive edges and negative edges\n",
    "2. **Testing**: test g_main on uknown positive edges and negative edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly permute edge ids\n",
    "edge_ids=np.arange(g.num_edges())\n",
    "edge_ids=np.random.permutation(edge_ids)\n",
    "\n",
    "train_size=int(0.8*g.num_edges())\n",
    "train_mask=edge_ids[:train_size]\n",
    "test_mask=edge_ids[train_size:]\n",
    "\n",
    "# g_main= original g with all test edges removed\n",
    "g_main=dgl.remove_edges(g,test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Obtain positive and negative samples\n",
    "1. Existing edges in graph = positive examples\n",
    "2. Non-existing edges (node pairs with no edges) = negative examples.\n",
    "3. Form train set and test set by the positive+negative examples\n",
    "4. Evaluate the model with AUC metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edges: 16888 | positive edges: 8444 | negative edges: 8444\n",
      "Test edges: 4224 | positive edges: 2112 | negative edges: 2112\n"
     ]
    }
   ],
   "source": [
    "# extract edges\n",
    "u,v=g.edges() # u=source nodes, v= destination nodes\n",
    "\n",
    "# (1) Get positive edges for training and testing of g_main\n",
    "train_pos_u, train_pos_v=u[train_mask], v[train_mask]\n",
    "test_pos_u, test_pos_v=u[test_mask], v[test_mask]\n",
    "\n",
    "# (2) Get negative edges for training and testing of g_main\n",
    "# create sparse adj matrix\n",
    "adj=sp.coo_matrix((np.ones(len(u)), (u.numpy(),v.numpy())))\n",
    "adj=adj.todense()+np.eye(g.num_nodes())\n",
    "\n",
    "u_neg, v_neg=np.where(adj==0)\n",
    "\n",
    "# sample g.num_edges() negative edges\n",
    "neg_ids=np.random.choice(len(u_neg),g.num_edges())\n",
    "train_neg_u,train_neg_v=u_neg[neg_ids[:train_size]], v_neg[neg_ids[:train_size]]\n",
    "\n",
    "test_neg_u,test_neg_v=u_neg[neg_ids[train_size:]], v_neg[neg_ids[train_size:]]\n",
    "\n",
    "print(f\"Train edges: {len(train_pos_u)+len(train_neg_v)} | positive edges: {len(train_pos_u)} | negative edges: {len(train_neg_u)}\")\n",
    "print(f\"Test edges: {len(test_pos_u)+len(test_neg_v)} | positive edges: {len(test_pos_u)} | negative edges: {len(test_neg_u)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 4 types of graphs for computing scores in training and testing\n",
    "\n",
    "# graphs used in computing scores in training\n",
    "# (1) positive train graph -> contain all positive edges\n",
    "g_train_pos=dgl.graph((train_pos_u,train_pos_v),num_nodes=g.num_nodes())\n",
    "\n",
    "# (2) negative train graph -> contain all positive edges\n",
    "g_train_neg=dgl.graph((train_neg_u,train_neg_v),num_nodes=g.num_nodes())\n",
    "\n",
    "\n",
    "\n",
    "# graphs used in computing scores in testing\n",
    "# (3) positive test graph -> contain all positive edges\n",
    "g_test_pos=dgl.graph((test_pos_u,test_pos_v),num_nodes=g.num_nodes())\n",
    "# (4) negative test graph -> contain all positive edges\n",
    "g_test_neg=dgl.graph((test_neg_u,test_neg_v),num_nodes=g.num_nodes())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GNN with SageConv\n",
    "dgl.nn.SAGEConv(in_dim, out_dim) updates in the following way\n",
    "\n",
    "\\begin{align*}\n",
    "h_i^{(l+1)}&= W.\\text{concat}(h_i^{(l)},h_{N(i)}^{(l+1)})+b \\ \\text{with} \\\\ h_{N(i)}^{(l+1)}&=\\text{Mean}\\{h_j^{(l)}, j\\in N(i)\\} \n",
    "\\end{align*}\n",
    "\n",
    "Here is our **model structure**\n",
    "<center>\n",
    "input -> SAGEConv1 -> relu -> SAGEConv2 -> predictor\n",
    "<end><center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "from sklearn.metrics import roc_auc_score # for computing auc metric\n",
    "\n",
    "class GraphSage(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.conv1=SAGEConv(in_dim,hidden_dim,aggregator_type=\"mean\")\n",
    "        self.conv2=SAGEConv(hidden_dim,hidden_dim,aggregator_type=\"mean\")\n",
    "    \n",
    "    def forward(self,g,features):\n",
    "        # g=input graph, features= input node features\n",
    "        h=self.conv1(g,features)\n",
    "        h=F.relu(h)\n",
    "        h=self.conv2(g,h)\n",
    "        return h\n",
    "    \n",
    "    def predict(self,g,h):\n",
    "        # compute dot product of src & dst features\n",
    "        #         store it as a new feature for g.edata\n",
    "        with g.local_scope():\n",
    "            g.ndata['h']=h\n",
    "            # create a new edge attribute 'score' = src_dot_dst\n",
    "            g.apply_edges(fn.u_dot_v('h','h','score'))\n",
    "            return g.edata['score'][:,0]\n",
    "        \n",
    "    def loss(self,pos_scores,neg_scores):\n",
    "        # pos_scores = scores for positive edges\n",
    "        # neg_scores = scores for negative edges\n",
    "        scores=torch.cat([pos_scores,neg_scores])\n",
    "        labels=torch.cat([torch.ones(pos_scores.shape[0]),torch.zeros(neg_scores.shape[0])])\n",
    "\n",
    "        return F.binary_cross_entropy_with_logits(scores,labels)\n",
    "    \n",
    "    def auc_score(self,pos_scores,neg_scores):\n",
    "        # roc_auc_score only accepts numpy array as inputs\n",
    "        scores=torch.cat([pos_scores,neg_scores]).detach().numpy()\n",
    "        labels=torch.cat([torch.ones(pos_scores.shape[0]),\n",
    "                          torch.zeros(neg_scores.shape[0])]).detach().numpy()\n",
    "        return roc_auc_score(labels,scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop: train graph g_main with information from g_train_pos and g_train_neg\n",
    "\n",
    "# train one epoch\n",
    "def train(model,g_main,g_train_pos,g_train_neg,optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    # forward and backward\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # prediction on g_main\n",
    "    h=model(g_main,g_main.ndata['feat']) # [num_nodes,feat_dim]\n",
    "\n",
    "    # prediction scores \n",
    "    pos_scores=model.predict(g_train_pos,h)\n",
    "    neg_scores=model.predict(g_train_neg,h)\n",
    "    loss=model.loss(pos_scores,neg_scores)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # compute auc\n",
    "    auc_score=model.auc_score(pos_scores,neg_scores)\n",
    "\n",
    "    return loss, auc_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model,g_main,g_test_pos, g_test_neg):\n",
    "    model.eval()\n",
    "    h=g_main.ndata['feat'] # these features are extracted after training is finished\n",
    "    \n",
    "    # forward\n",
    "    pos_scores=model.predict(g_test_pos,h)\n",
    "    neg_scores=model.predict(g_test_neg,h)\n",
    "\n",
    "    loss=model.loss(pos_scores,neg_scores)\n",
    "\n",
    "    auc_score=model.auc_score(pos_scores,neg_scores)\n",
    "\n",
    "    return loss,auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0464 million parameters\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "in_dim=g_main.ndata['feat'].shape[-1]\n",
    "hidden_dim=16\n",
    "num_epochs=100\n",
    "\n",
    "model=GraphSage(in_dim,hidden_dim)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | train_loss: 0.7260 |  train_auc: 0.5282  \n",
      "Epoch: 5 | train_loss: 0.6937 |  train_auc: 0.6533  \n",
      "Epoch: 10 | train_loss: 0.6889 |  train_auc: 0.6555  \n",
      "Epoch: 15 | train_loss: 0.6792 |  train_auc: 0.7847  \n",
      "Epoch: 20 | train_loss: 0.6559 |  train_auc: 0.7868  \n",
      "Epoch: 25 | train_loss: 0.6134 |  train_auc: 0.8098  \n",
      "Epoch: 30 | train_loss: 0.5766 |  train_auc: 0.8076  \n",
      "Epoch: 35 | train_loss: 0.5592 |  train_auc: 0.8183  \n",
      "Epoch: 40 | train_loss: 0.5414 |  train_auc: 0.8426  \n",
      "Epoch: 45 | train_loss: 0.5179 |  train_auc: 0.8680  \n",
      "Epoch: 50 | train_loss: 0.4943 |  train_auc: 0.8936  \n",
      "Epoch: 55 | train_loss: 0.4768 |  train_auc: 0.9097  \n",
      "Epoch: 60 | train_loss: 0.4603 |  train_auc: 0.9233  \n",
      "Epoch: 65 | train_loss: 0.4414 |  train_auc: 0.9350  \n",
      "Epoch: 70 | train_loss: 0.4243 |  train_auc: 0.9445  \n",
      "Epoch: 75 | train_loss: 0.4080 |  train_auc: 0.9520  \n",
      "Epoch: 80 | train_loss: 0.3927 |  train_auc: 0.9575  \n",
      "Epoch: 85 | train_loss: 0.3753 |  train_auc: 0.9630  \n",
      "Epoch: 90 | train_loss: 0.3577 |  train_auc: 0.9680  \n",
      "Epoch: 95 | train_loss: 0.3398 |  train_auc: 0.9724  \n",
      "Epoch: 99 | train_loss: 0.3256 |  train_auc: 0.9755  \n"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_auc=train(model,g_main,g_train_pos, g_train_neg,optimizer)\n",
    "    if epoch%5==0 or epoch==num_epochs-1:\n",
    "        print(f\"Epoch: {epoch} | train_loss: {train_loss:.4f} |  train_auc: {train_auc:.4f}  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.6913 | test_auc: 0.7948\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test graph\n",
    "with torch.no_grad():\n",
    "    test_loss, test_auc=evaluate(model,g_main,g_test_pos, g_test_neg)\n",
    "    print(f\"test_loss: {test_loss:.4f} | test_auc: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
