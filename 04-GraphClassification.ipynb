{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\n",
      "  Using cached ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from ogb) (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from ogb) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from ogb) (4.66.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from ogb) (1.5.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from ogb) (2.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from ogb) (2.2.2)\n",
      "Collecting outdated>=0.2.0 (from ogb)\n",
      "  Using cached outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools>=44 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (65.5.0)\n",
      "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
      "  Using cached littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: requests in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from pandas>=0.24.0->ogb) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
      "Requirement already satisfied: filelock in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from torch>=1.6.0->ogb) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from requests->outdated>=0.2.0->ogb) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/doductai/Youtube/dgl_tutorials/.venv/lib/python3.11/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
      "Using cached ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "Using cached outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Using cached littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
      "Installing collected packages: littleutils, outdated, ogb\n",
      "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ogb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset\n",
    "\n",
    "We will use GIN dataset: $1,113$ graphs (in 2 classes), each ranges between $[10,500]$ nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 2 | feature_dim: 3\n",
      "Dataset(\"PROTEINS\", num_graphs=1113, save_path=/Users/doductai/.dgl/PROTEINS_0c2c49a1)\n",
      "---- First graph -----\n",
      "(Graph(num_nodes=42, num_edges=204,\n",
      "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={}), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "dataset=dgl.data.GINDataset('PROTEINS',self_loop=True)\n",
    "\n",
    "print(f\"num_classes: {dataset.gclasses} | feature_dim: {dataset.dim_nfeats}\")\n",
    "print(dataset)\n",
    "\n",
    "print(\"---- First graph -----\")\n",
    "g=dataset[0]\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=109, num_edges=521,\n",
      "      ndata_schemes={'attr': Scheme(shape=(3,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "tensor([0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# prepare train and test set\n",
    "import numpy as np\n",
    "\n",
    "# data loader\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_train=int(0.8*len(dataset))\n",
    "\n",
    "# randomly permute train indices between [0,num_train]\n",
    "train_sampler=SubsetRandomSampler(torch.arange(num_train))\n",
    "# randomly permute test indices \n",
    "test_sampler=SubsetRandomSampler(torch.arange(num_train,len(dataset)))\n",
    "\n",
    "train_loader=GraphDataLoader(dataset,sampler=train_sampler,batch_size=4,drop_last=False)\n",
    "test_loader=GraphDataLoader(dataset,sampler=test_sampler,batch_size=4,drop_last=False)\n",
    "\n",
    "# print out a batch in train_loader\n",
    "batched_graph, labels=next(iter(train_loader))\n",
    "print(batched_graph)\n",
    "print(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random prediction: 0.20035938918590546\n"
     ]
    }
   ],
   "source": [
    "# random prediction on test set\n",
    "labels=dataset.labels\n",
    "labels_test=labels[num_train:]\n",
    "\n",
    "prob_random=sum(labels_test)/len(labels)\n",
    "print(f\"Random prediction: {prob_random}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GNN with GCN\n",
    "input -> AtomEncoder -> sequence[GraphConv + bn +  relu] -> mean_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder # encoder for atoms in molecular graph\n",
    "\n",
    "class GNN_GCN(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim,h_layers=8):\n",
    "        super().__init__()\n",
    "        self.node_encoder=AtomEncoder(hidden_dim)\n",
    "        self.h_layers=h_layers\n",
    "\n",
    "        # stack of GCNs\n",
    "        self.convs=nn.ModuleList([GraphConv(hidden_dim,hidden_dim) for _ in range(h_layers-1)])\n",
    "        self.convs.append(GraphConv(hidden_dim,out_dim)) # last layer\n",
    "\n",
    "        # stack of bn\n",
    "        self.bns=nn.ModuleList([torch.nn.BatchNorm1d(hidden_dim) for _ in range(h_layers-1)])\n",
    "    \n",
    "    def forward(self,g,x):\n",
    "        # node encoding\n",
    "        h=self.node_encoder(x)\n",
    "\n",
    "        # sequence of conv+bn+relu\n",
    "        for i in range(self.h_layers-1):\n",
    "            h=self.convs[i](g,h)\n",
    "            h=F.relu(self.bns[i](h))\n",
    "\n",
    "        # last conv\n",
    "        h=self.convs[-1](g,h)\n",
    "\n",
    "        # compute mean of all node features\n",
    "        g.ndata['h']=h\n",
    "        # out logits= mean_nodes\n",
    "        mean_feat=dgl.mean_nodes(g,'h') # [out_dim,]\n",
    "\n",
    "        return mean_feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041282 million parameters\n",
      "tensor([[-0.0256,  0.0489],\n",
      "        [-0.0254,  0.0486],\n",
      "        [-0.0242,  0.0550],\n",
      "        [-0.0252,  0.0497]])\n"
     ]
    }
   ],
   "source": [
    "in_dim=dataset.dim_nfeats\n",
    "hidden_dim=64\n",
    "out_dim=dataset.gclasses\n",
    "\n",
    "model=GNN_GCN(in_dim,hidden_dim,out_dim)\n",
    "print(f\"{sum(p.numel() for p in model.parameters())/1e6} million parameters\")\n",
    "\n",
    "# check 1 forward pass\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred_logits=model(batched_graph,batched_graph.ndata['attr'].long())\n",
    "print(pred_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.5978\n",
      "Epoch: 5 | Loss: 0.5446\n",
      "Epoch: 10 | Loss: 0.5172\n",
      "Epoch: 15 | Loss: 0.5318\n",
      "Epoch: 20 | Loss: 0.5436\n",
      "Epoch: 25 | Loss: 0.5356\n",
      "Epoch: 30 | Loss: 0.5281\n",
      "Epoch: 35 | Loss: 0.5270\n",
      "Epoch: 40 | Loss: 0.5197\n",
      "Epoch: 45 | Loss: 0.5241\n",
      "Epoch: 50 | Loss: 0.5184\n",
      "Epoch: 55 | Loss: 0.5180\n",
      "Epoch: 60 | Loss: 0.5282\n",
      "Epoch: 65 | Loss: 0.5206\n",
      "Epoch: 70 | Loss: 0.5130\n",
      "Epoch: 75 | Loss: 0.5217\n",
      "Epoch: 80 | Loss: 0.5042\n",
      "Epoch: 85 | Loss: 0.5215\n",
      "Epoch: 90 | Loss: 0.5225\n",
      "Epoch: 95 | Loss: 0.5152\n",
      "Epoch: 99 | Loss: 0.5158\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "model=GNN_GCN(in_dim,hidden_dim,out_dim)\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=4e-3)\n",
    "num_epochs=100\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total=0\n",
    "    for batched_graph,labels in train_loader:\n",
    "        logits=model(batched_graph,batched_graph.ndata['attr'].long())\n",
    "        loss=F.cross_entropy(logits,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update loss\n",
    "        loss_total+=loss.item()\n",
    "    loss_total/=len(train_loader)\n",
    "    if epoch%5==0 or epoch==num_epochs-1:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN-GCN | Test loss : 1.3148 | test accuracy=0.2287\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "num_correct, num_data, loss_total =0 ,0,0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batched_graph,labels in test_loader:\n",
    "        logits=model(batched_graph,batched_graph.ndata['attr'].long())\n",
    "        loss=F.cross_entropy(logits,labels)\n",
    "        loss_total+=loss.item()\n",
    "        num_correct += (logits.argmax(1)== labels).sum().item()\n",
    "        num_data+=len(labels)\n",
    "loss_total/=len(test_loader)\n",
    "test_acc=num_correct/num_data\n",
    "\n",
    "print(f\"GNN-GCN | Test loss : {loss_total:.4f} | test accuracy={test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GNN with SageConv\n",
    "input -> AtomEncoder -> sequence[SageConv + bn +  relu] -> mean_nodes -> classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class GNN_Sage(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,out_dim,h_layers=8):\n",
    "        super().__init__()\n",
    "        self.node_encoder=AtomEncoder(hidden_dim)\n",
    "        self.h_layers=h_layers\n",
    "\n",
    "        # stack of GCNs\n",
    "        self.convs=nn.ModuleList([SAGEConv(hidden_dim,hidden_dim, aggregator_type=\"mean\") for _ in range(h_layers-1)])\n",
    "        self.convs.append(SAGEConv(hidden_dim,out_dim, aggregator_type=\"mean\")) # last layer\n",
    "\n",
    "        # stack of bn\n",
    "        self.bns=nn.ModuleList([torch.nn.BatchNorm1d(hidden_dim) for _ in range(h_layers-1)])\n",
    "    \n",
    "    def forward(self,g,x):\n",
    "        # node encoding\n",
    "        h=self.node_encoder(x)\n",
    "\n",
    "        # sequence of conv+bn+relu\n",
    "        for i in range(self.h_layers-1):\n",
    "            h=self.convs[i](g,h)\n",
    "            h=F.relu(self.bns[i](h))\n",
    "\n",
    "        # last conv\n",
    "        h=self.convs[-1](g,h)\n",
    "\n",
    "        # compute mean of all node features\n",
    "        g.ndata['h']=h\n",
    "        # out logits= mean_nodes\n",
    "        mean_feat=dgl.mean_nodes(g,'h') # [out_dim,]\n",
    "\n",
    "        return mean_feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.6501\n",
      "Epoch: 5 | Loss: 0.5423\n",
      "Epoch: 10 | Loss: 0.5502\n",
      "Epoch: 15 | Loss: 0.5447\n",
      "Epoch: 20 | Loss: 0.5384\n",
      "Epoch: 25 | Loss: 0.5287\n",
      "Epoch: 30 | Loss: 0.5129\n",
      "Epoch: 35 | Loss: 0.5198\n",
      "Epoch: 40 | Loss: 0.5339\n",
      "Epoch: 45 | Loss: 0.5172\n",
      "Epoch: 50 | Loss: 0.5145\n",
      "Epoch: 55 | Loss: 0.5185\n",
      "Epoch: 60 | Loss: 0.5137\n",
      "Epoch: 65 | Loss: 0.5190\n",
      "Epoch: 70 | Loss: 0.5160\n",
      "Epoch: 75 | Loss: 0.5129\n",
      "Epoch: 80 | Loss: 0.5090\n",
      "Epoch: 85 | Loss: 0.5073\n",
      "Epoch: 90 | Loss: 0.5004\n",
      "Epoch: 95 | Loss: 0.5012\n",
      "Epoch: 99 | Loss: 0.5093\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1442)\n",
    "\n",
    "model=GNN_Sage(in_dim,hidden_dim,out_dim)\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=4e-3)\n",
    "num_epochs=100\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total=0\n",
    "    for batched_graph,labels in train_loader:\n",
    "        logits=model(batched_graph,batched_graph.ndata['attr'].long())\n",
    "        loss=F.cross_entropy(logits,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update loss\n",
    "        loss_total+=loss.item()\n",
    "    loss_total/=len(train_loader)\n",
    "    if epoch%5==0 or epoch==num_epochs-1:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN-Sage | Test loss : 1.1333 | test accuracy=0.3318\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "num_correct, num_data, loss_total =0 ,0,0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batched_graph,labels in test_loader:\n",
    "        logits=model(batched_graph,batched_graph.ndata['attr'].long())\n",
    "        loss=F.cross_entropy(logits,labels)\n",
    "        loss_total+=loss.item()\n",
    "        num_correct += (logits.argmax(1)== labels).sum().item()\n",
    "        num_data+=len(labels)\n",
    "loss_total/=len(test_loader)\n",
    "test_acc=num_correct/num_data\n",
    "\n",
    "print(f\"GNN-Sage | Test loss : {loss_total:.4f} | test accuracy={test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
